mcore_gpt: true
micro_batch_size: 1
global_batch_size: 128
tensor_model_parallel_size: 8
pipeline_model_parallel_size: 4
virtual_pipeline_model_parallel_size: null
encoder_seq_length: 131072
max_position_embeddings: 131072
num_layers: 32
hidden_size: 4096
ffn_hidden_size: 14336
num_attention_heads: 32
init_method_std: 0.02
use_scaled_init_method: true
hidden_dropout: 0.0
attention_dropout: 0.0
ffn_dropout: 0.0
kv_channels: null
apply_query_key_layer_scaling: true
normalization: rmsnorm
layernorm_epsilon: 1.0e-05
do_layer_norm_weight_decay: false
make_vocab_size_divisible_by: 128
pre_process: true
post_process: true
persist_layer_norm: true
bias: false
activation: fast-swiglu
headscale: false
transformer_block_type: pre_ln
openai_gelu: false
normalize_attention_scores: true
position_embedding_type: rope
rotary_percentage: 1.0
attention_type: multihead
share_embeddings_and_output_weights: false
overlap_p2p_comm: false
batch_p2p_comm: true
num_query_groups: 8
tokenizer:
  library: huggingface
  type: meta-llama/Meta-Llama-3.1-8B
  use_fast: true
native_amp_init_scale: 4294967296
native_amp_growth_interval: 1000
hysteresis: 2
fp32_residual_connection: false
fp16_lm_cross_entropy: false
megatron_amp_O2: true
grad_allreduce_chunk_size_mb: 125
grad_div_ar_fusion: true
gradient_accumulation_fusion: false
bias_activation_fusion: false
bias_dropout_add_fusion: false
masked_softmax_fusion: true
get_attention_mask_from_fusion: true
apply_rope_fusion: false
seed: 1234
resume_from_checkpoint: null
use_cpu_initialization: false
onnx_safe: false
apex_transformer_log_level: 30
gradient_as_bucket_view: false
sync_batch_comm: false
activations_checkpoint_granularity: full
activations_checkpoint_method: uniform
activations_checkpoint_num_layers: 1
num_micro_batches_with_partial_activation_checkpoints: null
activations_checkpoint_layers_per_pipeline: null
sequence_parallel: false
transformer_engine: true
fp8: false
fp8_e4m3: false
fp8_hybrid: true
fp8_margin: 0
fp8_interval: 1
fp8_amax_history_len: 1024
fp8_amax_compute_algo: max
reduce_amax: true
use_emha: false
data:
  chat: false
  chat_prompt_tokens:
    system_turn_start: "\0"
    turn_start: "\x11"
    label_start: "\x12"
    end_of_turn: '

      '
    end_of_name: '

      '
  sample: false
  num_workers: 0
  dataloader_type: single
  train_ds:
    file_path: /share5/users/song.wang/data/public_data/tulu-3-sft-mixture/packed_llama_8b_base/packed_train/packed_16384_seed0.npy
    global_batch_size: 128
    micro_batch_size: 1
    shuffle: true
    memmap_workers: null
    max_seq_length: 16384
    min_seq_length: 1
    drop_last: true
    label_key: output
    add_eos: true
    add_sep: false
    add_bos: false
    truncation_field: input
    index_mapping_dir: null
    prompt_template: '{input} {output}'
    hf_dataset: false
    truncation_method: right
    packed_sequence: true
  validation_ds:
    file_path: /share5/users/song.wang/data/public_data/tulu-3-sft-mixture/packed_llama_8b_base/packed_val_10k/packed_16384_seed0.npy
    global_batch_size: 128
    micro_batch_size: 1
    shuffle: false
    memmap_workers: null
    max_seq_length: 16384
    min_seq_length: 1
    drop_last: true
    label_key: output
    add_eos: true
    add_sep: false
    add_bos: false
    truncation_field: input
    index_mapping_dir: null
    prompt_template: '{input} {output}'
    hf_dataset: false
    truncation_method: right
    output_original_text: true
    packed_sequence: true
nsys_profile:
  enabled: false
  start_step: 10
  end_step: 10
  ranks:
  - 0
  gen_shape: false
optim:
  name: distributed_fused_adam
  lr: 5.0e-05
  weight_decay: 0.01
  betas:
  - 0.9
  - 0.98
  sched:
    name: CosineAnnealing
    warmup_steps: 10
    constant_steps: 1000
    min_lr: 9.0e-07
    max_steps: 1017
rotary_base: 500000.0
seq_len_interpolation_factor: 8.0
scale_positional_embedding: true
precision: bf16
target: nemo_aligner.models.nlp.gpt.gpt_sft_model.GPTSFTModel
nemo_version: 2.0.0
peft:
  peft_scheme: none
  restore_from_path: null
  lora_tuning:
    target_modules:
    - attention_qkv
    adapter_dim: 32
    adapter_dropout: 0.0
    column_init_method: xavier
    row_init_method: zero
    layer_selection: null
    weight_tying: false
    position_embedding_strategy: null
answer_only_loss: true
restore_from_path: /share5/projects/llm/models/weight/Llama-3.1-8B.nemo
save_nemo_on_validation_end: true
use_flash_attention: true
pipeline_model_parallel_split_rank: 0
inference: {}
